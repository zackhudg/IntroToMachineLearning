{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> \n",
    "        \n",
    "‚ùóBefore starting, make sure **hw2_data.pgz** and **images_5_classes.tar.gz** are in the same directory as your notebook.<br>\n",
    "‚ùóRun the below cell to unzip the data. (Needed for displaying images in \"HW2 Description\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if not os.path.isdir('images_5_classes'): \n",
    "    !tar -xvzf images_5_classes.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 Description\n",
    "\n",
    "In this homework assignment, we are working with instagram photos of **five national parks**. The task is to train up a classifier that correctly predicts the park given a photo. This is a non-trivial task, since the naive approach (using each pixel as a feature) will fail. To see why consider the two photos below.\n",
    "\n",
    "| Arches 1| Arches 2 |\n",
    "|-|-|\n",
    "|![](./images_5_classes/archesnationalpark/2_1342855034455221159.jpg)|![](./images_5_classes/archesnationalpark/2_1343181954723713884.jpg)|\n",
    "\n",
    "Landmarks occur in both photos but not in the same position in either image. Further, due to the opening in the arch, the middle of the photo may be occupied by sky or alternatively by rock. At best, treating pixels as features we can analyze the images in terms of color frequencies (i.e. the intensities of the red, green, and blue channels).\n",
    "\n",
    "Instead of naively using each pixel as a feature, we will provide you with **precomputed feature vectors**, computed by a neural network known as **AlexNet**. These features provide aggregated information about edges, shapes, and color intensities across each image. \n",
    "\n",
    "Hence, your task is to train a prediction model which takes as input distilled representations of the photos (the 1000-dimensional feature vectors provided by AlexNet) and predicts where (one of the five National Parks) the photo was taken. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "    kwargs = {}\n",
    "except:\n",
    "    import _pickle as pickle\n",
    "    kwargs = {'encoding':'bytes'}\n",
    "\n",
    "features, labels, sample_ids, label_names =  pickle.load( gzip.open('hw2_data.pgz', \n",
    "                                                                    'rb'), **kwargs )\n",
    "print('IDs and the corresponding national parks:')\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(12345)\n",
    "N = features.shape[0]\n",
    "arr = np.arange(N)\n",
    "np.random.shuffle(arr)\n",
    "train_num = int(round(N*0.8))\n",
    "test_num = features.shape[0]-train_num\n",
    "\n",
    "train_subset = arr[:train_num]\n",
    "train_features = features[train_subset,:]\n",
    "train_labels = labels[train_subset]\n",
    "train_sample_ids = [sample_ids[i] for i in train_subset]\n",
    "\n",
    "test_subset = arr[train_num:]\n",
    "test_features = features[test_subset,:]\n",
    "test_labels = labels[test_subset]\n",
    "test_sample_ids = [sample_ids[i] for i in test_subset]\n",
    "print(\"Dataset split into {} training and {} test examples.\".format(train_num,test_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The plan:\n",
    "\n",
    "We will model features of the dataset using **<span style='background: greenyellow'> Bayesian networks</span>**, one network for each of the five classses (National Parks). Specifically, we will be using <span style='background: greenyellow'>Bayesian trees</span>. <br><br>\n",
    "$$\n",
    "p(\\mathbf{x} \\mid h=c) = \\prod_{(j,k) \\in \\textrm{Edges}^c} p(x_j \\mid x_k,\\theta_{j}^c)\n",
    "$$\n",
    "where $\\textrm{Edges}^c$ is the edge set describing the tree for class $c$ and $\\theta_{j}^c$ are parameters for conditional probability of $x_j$ given its parent $x_k$. <span style='background: greenyellow'>*Note that different classes will correspond to different trees and use different parameters!*</span>\n",
    "\n",
    "### The tasks we will have to accomplish are:\n",
    "1. <a href='#1.-Learning-a-tree-structure-(1pt)'>Implement learning of the tree structure for each class using Chow-Liu algorithm (**1pt**) </a>\n",
    "2. <a href='#2.-Learning-parameters-of-a-tree-structured-Bayes-net-(3pt)'>Learn parameters for conditional probabilities associated with edges in these trees (**3pts**)</a>\n",
    "3. <a href=\"#3.-Compute-probability-of-a-feature-vector-x-given-a-Bayes-net's-structure-and-parameters-(2pt)\">Compute probability of a feature vector $\\mathbf{x}$ in each of the five Bayesian networks (**2pt**) </a>\n",
    "4. <a href='#4.-Compute-probabilities-that-a-feature-vector-x-belongs-to-each-class-(2pt)'>Compute probabilities that the feature vector $\\mathbf{x}$  belongs to each class (**2pt**) </a>\n",
    "5. <a href='#5.-Make-predictions-based-on-probabilities-(1pt)'>Make predictions based on probabilities (**1pt**)</a>\n",
    "6. <a href='#6.-Explore-prediction-performance-(1pt)'>Explore prediction performance (**1pt**) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable/function definitions for the code aspect of this homework: \n",
    "|Variable Name | Meaning |\n",
    "|-|-|\n",
    "| {```train```&#124;```test```}```features``` | Matrix of features of photos |\n",
    "| {```train```&#124;```test```}```labels```  | Vector of labels of photos |\n",
    "| {```train```&#124;```test```}```sample_ids``` | List of ids for each sample (filenames) |\n",
    "| ```label_names``` | Names of classes (list of strings) |\n",
    "| ```class_edges``` | Dictionary of edge lists. One edge list per class |\n",
    "| ```class_thetas``` | Dictionary of parameters. One theta per class |\n",
    "\n",
    "| Function name | Purpose |\n",
    "|-|-|\n",
    "|```corr``` | Computes Pearson correlation between two vectors. |\n",
    "| ```mutual_information``` | Computes mutual information between two vectors. |\n",
    "|```mutual_info_all```| Compute mutual information between columns of  a matrix. |\n",
    "|```chowliu``` | Chow-Liu algorithm on a feature matrix. <br>Returns adjacency matrix; non-zero entry indicates edge. |\n",
    "|```get_edge_list``` | Creates an edge list from adjacency matrix. <br>Each edge represented by (parent,child) element. |\n",
    "|```get_label_subsets``` | Get sample indices for each class. Returns a dictionary of lists. <br \\>Key is class label $c$, value is sample indices for that class |\n",
    "|```get_edges_for_each_class```| Produces an edge list for each class. Returns a dictionary of lists.<br>Key is class label $c$, value is edge set for that class $\\textrm{Edges}^c$|\n",
    "|```get_thetas_for_each_class```| Learns parameters for each class. Returns a dictionary of matrices. <br>Key is class label $c$, value is matrix $\\theta^c$.| \n",
    "|```p_h_given_x_theta```| Computes probability that particular sample $x$ came from a class $h$ <br>given all the class edge sets and parameters. Uses Bayes rule.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "\n",
    "# 1. Learning a tree structure  <span style='background :yellow' > (1pt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"border:2px solid deepskyblue\"> </hr>\n",
    "\n",
    "### 1a) Computing mutual information\n",
    "\n",
    "Chow-Liu structure learning algorithm uses **<span style='background :greenyellow'>mutual information</span>** to find the best tree. In class, we specified how to compute mutual information between variables distributed according to a categorical distribution. If we assume that the variables are **Gaussian**, then mutual information is given \n",
    "by\n",
    "\n",
    "$$ \n",
    "I(X,Y) = -\\frac{1}{2} \\ln(1-Corr(X, Y)^2) \n",
    "$$\n",
    "where *Corr* denotes <span style='background :greenyellow'>**Pearson correlation**</span>.\n",
    "<br>\n",
    "\n",
    "\n",
    "<font color=blue> **ToDo**\n",
    "* Implement mutual information computation in the function ```mutual_information```. \n",
    "    (Hint: Use the ```corr``` function we have provided.)\n",
    "* Implement the function ```mutual_info_all``` which computes mutual information between all features (columns) in a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(x,y):\n",
    "    if x.shape[0] == 0:\n",
    "        rowvar = 1\n",
    "    else:\n",
    "        rowvar = 0\n",
    "    return np.corrcoef(np.asarray(x),np.asarray(y),rowvar = rowvar)[0,1]\n",
    "\n",
    "def mutual_information(x,y):\n",
    "    return ...    ## FILL-IN-THE-BLANK ##\n",
    "\n",
    "def mutual_info_all(M):\n",
    "    f_num = M.shape[1] # number of features\n",
    "    mi_ary = np.zeros( (f_num, f_num) )\n",
    "    ...           ## FILL-IN-THE-BLANK ##\n",
    "    ...           ## FILL-IN-THE-BLANK ##\n",
    "    ...           ## FILL-IN-THE-BLANK ##\n",
    "    return mi_ary\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(10)\n",
    "y = x + 0.01*np.random.randn(10)\n",
    "z = -x + 0.01*np.random.randn(10)\n",
    "\n",
    "print((\"Mutual information {:1.3f} between variables \"+\n",
    "      \"with correlation {:1.3f}.\").format(mutual_information(x,y),corr(x,y)))\n",
    "print((\"Mutual information {:1.3f} between variables \"+\n",
    "      \"with correlation {:1.3f}.\").format(mutual_information(x,z),corr(x,z)))\n",
    "features = np.asmatrix([x,y,z]).transpose()\n",
    "MI = mutual_info_all(features)\n",
    "\n",
    "print(MI,(\"\\nMI(x,z) = {:1.3f} should be larger \" + \n",
    "       \"than MI(y,z) = {:1.3f}.\").format(MI[0,2],MI[1,2]))\n",
    "assert(MI[0,2]>MI[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ If you did everything correctly, highly correlated variables,```x``` and ```y```, should have **high** mutual information. <br>\n",
    "‚úÖ Similarly highly anti-correlated variables,  ```z``` and ```x```, should have **high** mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid deepskyblue\"> </hr>\n",
    "\n",
    "### 1b) Chow-Liu tree algorithm\n",
    "\n",
    "Given a feature matrix, the matrix of mutual information (denoted as ```M```) between features, pairwise, provides <span style='background :greenyellow'> weights for each candidate edge</span> in a Bayesian network. Running the **<span style='background :greenyellow'>maximum spanning tree</span>** algorithm on this weight matrix yields optimal connectivity structure of the graph. \n",
    "\n",
    "<font color=blue> **ToDo** *(In order to implement the Chow-Liu algorithm, you will need to do the following... )*\n",
    "\n",
    "* Compute mutual information between columns (features).\n",
    "* Compute maximum spanning tree on mutual information matrix. \n",
    "     [Hint: Use ```scipy```'s implementation of ***minimum*** spanning tree on matrix of ***negative*** mutual information.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "def chowliu(features):\n",
    "    ...    ## FILL-IN-THE-BLANK ##\n",
    "    ...    ## FILL-IN-THE-BLANK ##\n",
    "    return adjacency_matrix\n",
    "\n",
    "def get_edge_list(mat):\n",
    "    f_num = mat.shape[0]\n",
    "    edges = []\n",
    "    for k in range(f_num):\n",
    "        lst = np.nonzero(mat[k,:])[1]\n",
    "        # k is parent, j is child\n",
    "        new_edges = [(k,j) for j in lst]        \n",
    "        edges.extend(new_edges)\n",
    "    return edges\n",
    "        \n",
    "np.random.seed(1)\n",
    "x = np.random.randn(20)\n",
    "y = x + 0.1*np.random.randn(20)\n",
    "z = -x + 0.1*np.random.randn(20)\n",
    "q = z + 0.1*np.random.randn(20)\n",
    "w = y + 0.1*np.random.rand(20)\n",
    "features = np.asmatrix([x,y,z,q,w]).transpose()\n",
    "adjacency = chowliu(features)\n",
    "names = ['x','y','z','q','w']\n",
    "edges = get_edge_list(adjacency)\n",
    "print('Edge list: ',[(names[i] + '->' + names[j]) for (i,j) in edges])\n",
    "\n",
    "edges = get_edge_list(adjacency)\n",
    "\n",
    "assert(set(edges)== set([(0, 1), (0, 2), (1, 4), (2, 3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ Sanity check for tree building implementation\n",
    "\n",
    "  Edge pairs between ```x,y,z,q,w``` should reflect the dependencies of variables in the code.\n",
    "\n",
    "Since  \n",
    "```pyhon\n",
    "y = x + 0.1*np.random.randn(20)\n",
    "```\n",
    "```y``` is a noisy version of ```x``` and we expect to have an edge ```x->y```. Similarly, since\n",
    "```\n",
    "w = y + 0.1*np.random.rand(20)\n",
    "```\n",
    "```w``` is a noisy version of ```y``` and we expect\n",
    "to see edges ```y->w```.\n",
    "\n",
    "\n",
    "\n",
    "If you did everything correctly. You should output an edge list as follows...\n",
    "\n",
    "```Edge list: ['x->y', 'x->z', 'y->w', 'z->q']```\n",
    "\n",
    "... also, the ```assert``` should pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid deepskyblue\"> </hr>\n",
    "\n",
    "### 1c) Learning class-specific trees\n",
    "\n",
    "We need to train a Bayesian tree for *each* class! <br>\n",
    "\n",
    "<br>\n",
    "<font color=blue> \n",
    "    \n",
    "**ToDo**\n",
    "* Split training data into class-specific data matrices.\n",
    "* Run the Chow-Liu tree algorithm on class-specific data subsets and obtain resulting edge lists.\n",
    "* Store resulting edge lists, one per class, in the dictionary ```class_edges```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_subsets(train_labels):\n",
    "    label_set = np.unique(train_labels) #get 5 label numbers\n",
    "    label_sample_map = {} #a label to sample index map\n",
    "    for ...        ## FILL-IN-THE-BLANK ##\n",
    "        ...        ## FILL-IN-THE-BLANK ##\n",
    "    return label_sample_map\n",
    "        \n",
    "def get_edges_for_each_class(train_features, train_labels):\n",
    "    label_set = np.unique(train_labels) #get 5 label numbers\n",
    "    label_sample_map = get_label_subsets(train_labels)    \n",
    "    class_edges = {}\n",
    "    for ...         ## FILL-IN-THE-BLANK ##\n",
    "        ...         ## FILL-IN-THE-BLANK ##\n",
    "        ...         ## FILL-IN-THE-BLANK ##\n",
    "    return class_edges\n",
    "    \n",
    "class_edges = get_edges_for_each_class(train_features, train_labels)    \n",
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "\n",
    "# 2. Learning parameters of a tree-structured Bayes net <span style='background :yellow' > (3pt)\n",
    "</span>\n",
    "\n",
    "Given a tree structure, we can write out the log-likelihood for a Bayes net. In a tree, a node has at most one parent.\n",
    "\n",
    "We will denote the set of edges in a tree as Edges. Hence if node $k$ is parent of node $j$ then the pair $(j,k)$ will be in the set Edges. *Note that if edge is $(j,k)$ is in the set Edges, then $(k,j)$ will not be.* We will denote the node that has no parents (the **root** node) as $r$.\n",
    "\n",
    "Log-likelihood is given by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LL(\\Theta) &= \\underbrace{\\sum_{i=1}^N}_{\\textrm{samples}}\\left[ \\sum_{(j,k) \\in \\textrm{Edges}} \\log p(x_{i,j} \\mid x_{i,k},\\theta_{j}) + \\log p(x_{i,r}\\mid\\theta_{r})\\right]\\\\\n",
    "&= \\sum_{(j,k) \\in \\textrm{Edges}} \\sum_{i=1}^N\\log p(x_{i,j} \\mid x_{i,k},\\theta_{j}) + \\sum_{i=1}^N\\log p(x_{i,r}\\mid\\theta_{r}) \n",
    "\\end{aligned}.\\tag{1}\n",
    "$$\n",
    "\n",
    "Since $\\theta_{j}$ only appears only in terms involving edge $(j,k)$ we can eliminate other terms and obtain  optimal $\\theta_{j}$ as\n",
    "$$\n",
    "\\theta_j^* = \\mathop{\\textrm{argmax}}_{\\theta_j}\\sum_{i=1}^N\\log p(x_{i,j} \\mid x_{i,k},\\theta_{j}).\n",
    "$$\n",
    "Similarly, the optimal $\\theta_{r}$ is given by\n",
    "$$\n",
    "\\theta_r^* = \\mathop{\\textrm{argmax}}_{\\theta_r}\\sum_{i=1}^N\\log p(x_{i,r} \\mid \\theta_r).\n",
    "$$\n",
    "Put succinctly, all of these optimization problems can be <span style='background :greenyellow'> solved separately</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr style=\"border:2px solid deepskyblue\"> </hr>\n",
    "\n",
    "### 2a) Parameter learning\n",
    "\n",
    "We need to specify how the child variables depend on their parents. Since our features are continuous, we will assume that the conditional probabilities $p(x_j \\mid x_k)$ are Gaussian. Each feature will be modeled as a <span style='background :greenyellow'> linear function of its parent</span> with a bias contribution. Since root does not have a parent, it will have just a bias parameter.\n",
    "\n",
    "Specifically,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_j &\\mid x_k,\\theta_j \\sim \\mathcal{N}(\\theta_{j,0} + \\theta_{j,1}x_k,\\sigma^2) \\\\\n",
    "x_r &\\mid \\theta_r \\sim \\mathcal{N}(\\theta_{r,0},\\sigma^2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Log conditional probabilities are given by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(x_j\\mid x_k,\\theta_j) &= -\\log 2\\pi \\sigma^2 -  \\frac{1}{2\\sigma^2}(x_j - \\theta_{j,0} - \\theta_{j,1}x_{k})^2\\\\\n",
    "\\log p(x_r\\mid \\theta_r) &= -\\log 2\\pi \\sigma^2 -  \\frac{1}{2\\sigma^2}(x_j - \\theta_{r,0} )^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "<br>\n",
    "Hence, terms in log-likelihood (Eq.1) involving $\\theta_j$ are\n",
    "$$\n",
    "\\sum_{i=1}^N \\left[-\\log 2\\pi \\sigma^2 -  \\frac{1}{2\\sigma^2}(x_{i,j} - \\theta_{j,0} - \\theta_{j,1}x_{i,k})^2 \\right] \\tag{2}\n",
    "$$\n",
    "and  terms involving $\\theta_r$ are\n",
    "$$\n",
    "\\sum_{i=1}^N \\left[-\\log 2\\pi \\sigma^2 -  \\frac{1}{2\\sigma^2}(x_{i,r} - \\theta_{r,0} )^2 \\right]  \\tag{3}.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain optimal parameters for the model <span style='background :greenyellow'> we will maximize Eq. 2 and Eq. 3 with regard to parameters $\\theta$. </span>\n",
    "\n",
    "***We will not obtain a closed form solution for $\\theta_{j,0}$ nor $\\theta_{j,1}$!*** Rather, we will <span style='background :greenyellow'> iterate updates </span> which fix $\\theta_{j,0}$ to get best $\\theta_{j,1}$ and then fix $\\theta_{j,1}$ to get best $\\theta_{j,0}$. This approach is called **<span style='background :greenyellow'>coordinate ascent</span>**. \n",
    "\n",
    "<br> \n",
    "<font color=blue>\n",
    "\n",
    "**ToDo**\n",
    "\n",
    "* Take derivatives of the Eq. 2 and Eq. 3 above with respect to $\\theta_j$ and $\\theta_r$. Then, equate those derivatives to zero and solve for $\\theta_{j,0},\\theta_{j,1}$ and $\\theta_{r,0}$. \n",
    "    <br>\n",
    "\n",
    "$$ \\large\n",
    "\\begin{aligned}\n",
    "\\theta^*_{j,0} &= \\frac{\\sum_{i=1}^N ...}{...}\\\\\\\\\n",
    "\\theta^*_{j,1} &= \\frac{\\sum_{i=1}^N ...}{\\sum_{i=1}^N ...}\\\\\\\\\n",
    "\\theta^*_{r,0} &= \\frac{\\sum_{i=1}^N ... }{...}\\\\\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "<font color=black>\n",
    "    \n",
    " * Note that  $\\theta^*_{j,0}$ depends on $\\theta_{j,1}$ and vice versa. This suggests that we need to *iterate* these updates until there is no more change in either variable! However $\\theta_{r,0}$ can be solved in a *single* update.\n",
    "\n",
    "<br>\n",
    "<br> \n",
    "<font color=blue>\n",
    "\n",
    "* Implement function that finds optimal $\\theta_j$ given vectors $x_j$ and $x_k$. \n",
    "<br>   \n",
    "* Implement function that finds optimal $\\theta_r$ given vector of $x_r$. Our code assumes that r=0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update for ùúÉ_{r,0}, r is the root_index (0 in our case)\n",
    "def compute_theta_r( x ):\n",
    "    return ... ## FILL-IN-THE-BLANK ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update for ùúÉ_{j, 0}, for link between j and k given ùúÉ_j_1. \n",
    "#k is parent, j is child\n",
    "def compute_theta_j_k_0( j, k, x, theta_j_1 ):\n",
    "    return ... ## FILL-IN-THE-BLANK ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update for ùúÉ_{j, 1}, for link between j and k given ùúÉ_j_0. \n",
    "#k is parent, j is child\n",
    "def compute_theta_j_k_1( j, k, x, theta_j_0 ):\n",
    "    return ... ## FILL-IN-THE-BLANK ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ In the below code, we will check that the code you wrote accurately estimates $\\theta_r$ and $\\theta_j$s. The true parameters are $\\theta_r = 1.0$, $\\theta_{1,0} = 0.5$ and $\\theta_{1,1} = 2.0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1000\n",
    "x = np.zeros((d,2))\n",
    "x[:,0] = 1.0 + np.random.randn(d,)\n",
    "x[:,1] = 0.5 + 2.0*x[:,0] + np.random.randn(d,)\n",
    "theta_j = [0.0,0.0]\n",
    "\n",
    "theta_r = compute_theta_r( x )\n",
    "for it in range(40):\n",
    "    theta_j[0] = compute_theta_j_k_0( 1, 0, x, theta_j[1] )\n",
    "    theta_j[1] = compute_theta_j_k_1( 1, 0, x, theta_j[0])\n",
    "print(\"theta_r_0:{} theta_j:{}\".format(theta_r,theta_j))    \n",
    "assert(abs(theta_r - 1.0)<0.1)\n",
    "assert(abs(theta_j[0] - 0.5)<0.1)\n",
    "assert(abs(theta_j[1] - 2.0)<0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid deepskyblue\"> </hr>\n",
    "\n",
    "### 2b)  Training the model\n",
    "Compute $\\theta^*_j$ and $\\theta^*_r$ for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thetas_for_each_class(train_features, train_labels, class_edges):\n",
    "    label_sample_map = get_label_subsets(train_labels)\n",
    "    label_set = np.unique(train_labels)\n",
    "    class_thetas = {}\n",
    "    f_num = train_features.shape[1]\n",
    "    for lab in label_set:\n",
    "        print( 'processing class label {}'.format(lab) )\n",
    "        c_samples = train_features[...]    ## FILL-IN-THE-BLANK ##\n",
    "        theta_r = compute_theta_r( ... )   ## FILL-IN-THE-BLANK ##\n",
    "        c_edge_list= class_edges[...]      ## FILL-IN-THE-BLANK ##\n",
    "        thetas = np.zeros((f_num,2)) #the first column is j_0, and the second column is  j_1\n",
    "        #the first row (thetas[0,:]) is for ùúÉ_r\n",
    "\n",
    "        for (k,j) in c_edge_list:        \n",
    "            theta_j_1 = 0\n",
    "            # do coordinate ascent using compute_theta_j_k_0 and compute_theta_j_k_1\n",
    "            for z ...                      ## FILL-IN-THE-BLANK ##\n",
    "                ...                        ## FILL-IN-THE-BLANK ##\n",
    "                ...                        ## FILL-IN-THE-BLANK ##\n",
    "            #set the optimal ùúÉ_j_0 and ùúÉ_j_1 for this the edge (k, j)\n",
    "            thetas[j, 0] = ...             ## FILL-IN-THE-BLANK ##\n",
    "            thetas[j, 1] = ...             ## FILL-IN-THE-BLANK ##\n",
    "        thetas[0,0] = theta_r   \n",
    "        thetas[0,1] = np.nan # root has no parents\n",
    "        class_thetas[lab] = thetas\n",
    "    return class_thetas\n",
    "\n",
    "class_thetas = get_thetas_for_each_class(train_features,train_labels,class_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "\n",
    "# 3. Compute probability of a feature vector x given a Bayes net's structure and parameters <span style='background :yellow' > (2pt)\n",
    "\n",
    "<font color=blue> \n",
    "<br> \n",
    "    \n",
    "**ToDo** \n",
    "    \n",
    "* Implement function that computes $\\log p(x_j\\mid x_k,\\theta_j)$ for specific edge using optimal $\\theta^*_j$. \n",
    "* Implement a function that computes $\\log p(x_r\\mid \\theta_r)$ using optimal $\\theta^*_r$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log p(x_j|x_k, ùúÉ_j), for the edge between j and k, given ùúÉ_{j, 0} and ùúÉ_{j,1}\n",
    "# Let sigma^2 = 1\n",
    "def compute_lp_j_k( j, k, x, theta_j_0, theta_j_1 ):\n",
    "    return -... - ...    ## FILL-IN-THE-BLANK ##\n",
    "\n",
    "# compute log p(x_r|ùúÉ_r), for the root node, given ùúÉ_r. \n",
    "def compute_lp_r( x, theta_r ):\n",
    "    return -... - ...    ## FILL-IN-THE-BLANK ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> \n",
    "\n",
    "**ToDo** \n",
    "\n",
    "* Implement a function that computes the log-probability of a sample $\\mathbf{x}$ given a Root, Edges, and $\\Theta$\n",
    "</font>\n",
    "    $$\n",
    "\\log p(\\mathbf{x} \\mid \\Theta^*) = \\sum_{r \\in \\textrm{Roots}} \\log p(x_r \\mid \\theta^*_r) + \\sum_{(j,k) \\in \\textrm{Edges}} \\log p(x_j \\mid x_k, \\theta^*_{j})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lp_x_given_Theta( x, thetas, edges):\n",
    "    lp = compute_lp_r( ... )    ## FILL-IN-THE-BLANK ##    \n",
    "    for (k,j) in edges:  \n",
    "        # k is parent, j is child\n",
    "        lp = ...                ## FILL-IN-THE-BLANK ##\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "\n",
    "# 4. Compute probabilities that a feature vector x belongs to each class <span style='background :yellow' > (2pt)\n",
    "Tree-augmented Naive Bayes uses following model\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(h = c) &= \\pi_c\\\\\n",
    "p(\\mathbf{x} \\mid \\Theta, h) &= p(\\mathbf{x} \\mid \\Theta_c) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "<br>\n",
    "where $\\Theta_c$ are parameters of the Bayes net for feature vector $\\mathbf{x}$ trained on class $c$.\n",
    "<br>\n",
    "<br>\n",
    "<font color=blue>\n",
    "\n",
    "**ToDo** <br>\n",
    "* Use Bayes rule to express the posterior probability of class $c$ given the data $\\mathbf{x}$ and model parameters $\\theta$.\n",
    "    <br><br>\n",
    "    \n",
    "$$ \\large\n",
    "p(h = c | \\mathbf{x},\\Theta) = \\frac{ p(...)p(...) }{ \\sum_{d} p(...)p(...) } \\\\\\\\\n",
    "$$\n",
    "    <br>\n",
    "* Implement a function that takes as input $\\Theta$ and a sample $\\mathbf{x}$ and computes $p(h\\mid\\mathbf{x},\\Theta)$. This function will use the expression you derived above. <font color=black>\n",
    " * Note that ```compute_lp_x_given_Theta``` computes log-probabilty of $\\mathbf{x}$ for a class-specific tree, $\\log p(\\mathbf{x}\\mid h=c,\\Theta)$. Below, we provide code that computes $\\log p(h)$. You just need to put together these pieces of code, making sure to use ```logsumexp``` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_p_h(train_labels):\n",
    "    label_set = np.unique(train_labels)\n",
    "    log_p_h = np.zeros(len(np.unique(labels)))\n",
    "    for i in label_set:\n",
    "        count = len(np.nonzero(train_labels==i)[0])\n",
    "        log_p_h[i] = np.log( float(count) / float(len(train_labels) ) )\n",
    "    return log_p_h\n",
    "\n",
    "log_p_h = get_log_p_h(train_labels)        \n",
    "\n",
    "def logsumexp(vec):\n",
    "    m = np.max(vec,axis=0)   \n",
    "    return np.log(np.sum(np.exp(vec-m),axis=0))+m\n",
    "\n",
    "def p_h_given_x_theta( x, class_edges, class_thetas, log_p_h ):\n",
    "    \n",
    "    C = len(class_thetas)\n",
    "    lognumerator = np.zeros(C)\n",
    "    \n",
    "    # implement Bayes rule here\n",
    "    # compute log-numerators first and then normalize using logsumexp\n",
    "    for i in range(C): \n",
    "        edges = class_edges[i]\n",
    "        thetas = class_thetas[i]        \n",
    "        lognumerator[i] = compute_lp_x_given_Theta(...) + ...    ## FILL-IN-THE-BLANK ##\n",
    "    \n",
    "    # use logsumexp to compute denominator\n",
    "    logdenominator = ...                                         ## FILL-IN-THE-BLANK ##\n",
    "    probs = ...                                                  ## FILL-IN-THE-BLANK ##\n",
    "    \n",
    "    assert(np.all(probs >= 0))\n",
    "    assert(np.abs(np.sum(probs)-1.0)<1e-5)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "\n",
    "# 5. Make predictions based on probabilities <span style='background :yellow' > (1pt)\n",
    "\n",
    "<br>\n",
    "<font color=blue> \n",
    "\n",
    "**ToDo**   \n",
    "* Make predictions on the test set using parameters ```class_thetas```, tree structures ```class_edges```, and log-prob ```log_p_h``` as input to function ```p_h_given_x_theta```. Given the function's output, select the most probable class for each sample and store corresponding probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(test_features,test_labels,class_edges,class_thetas,log_p_h):\n",
    "    label_set = np.unique(train_labels)\n",
    "    pred_lab = np.zeros((test_features.shape[0], 2)) \n",
    "    #the 1st column is the predicted label, and the 2nd column is probability of that label.\n",
    "    for i in range(test_num):\n",
    "        x = test_features[i,:]    \n",
    "        res =  p_h_given_x_theta( ... )    ## FILL-IN-THE-BLANK ##     \n",
    "        # predicted label\n",
    "        pred_lab[i, 0] = ...               ## FILL-IN-THE-BLANK ##\n",
    "        # probability of that label\n",
    "        pred_lab[i, 1] = ...               ## FILL-IN-THE-BLANK ##\n",
    "    return pred_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lab = evaluate_predictions(test_features,test_labels,class_edges,class_thetas,log_p_h)\n",
    "print(\"Prediction Accuracy: {}\".format(np.mean(pred_lab[:,0]==test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "<hr style=\"border:4px solid blue\"> </hr>\n",
    "\n",
    "# 6. Explore prediction performance <span style='background :yellow' > (1pt)\n",
    "\n",
    "Lastly, we are going to evaluate <span style='background :greenyellow'> how frequently photos from one park get mistaken from photos from another</span> using a <span style='background :greenyellow'>**confusion matrix**</span>. An entry of the confusion matrix in $i$th row and $j$th column counts how many photos are in class $i$ but were predicted to be in class $j$. The diagonal is the count of correct predictions. Off-diagonal entries are counts of specific types of mistakes (i.e. Type I and Type II). \n",
    "\n",
    "A confusion matrix is *not* symmetric! (e.g. Mistaking apples for oranges is not the same as mistaking oranges for apples.)\n",
    "<br>\n",
    "<font color=blue> \n",
    "\n",
    "**ToDo**\n",
    " * To earn this point, you just need to run the cells below and produce the outputs. üî•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, rotation_mode='anchor', ha = 'right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(test_labels, pred_lab[:, 0])\n",
    "label_set = np.unique(test_labels)\n",
    "plot_confusion_matrix(cnf_matrix, [label_names[i].decode() for i in label_set],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "def gen_err_predicted_fig( test_labels, pred_lab, label_set, label_names, \n",
    "                           test_sample_ids, srcPath='images_5_classes', \n",
    "                          figsize=(10, 10)):\n",
    "    errNameMap = {}\n",
    "    maxErrNameMap = {}\n",
    "    cnt = 0\n",
    "    for lab in label_set:\n",
    "        for plab in label_set:\n",
    "            if lab == plab:\n",
    "                continue\n",
    "            else:\n",
    "                cIdx = np.nonzero(test_labels==lab)[0]\n",
    "                errIdx = cIdx[ np.nonzero(pred_lab[cIdx, 0]==plab)[0] ]\n",
    "                errNameMap[(lab, plab)] = errIdx\n",
    "                if len(errIdx) != 0:\n",
    "                    errProb = np.max( pred_lab[errIdx, 1] )\n",
    "                    errFIdx = errIdx[ np.argmax( pred_lab[errIdx, 1] ) ]\n",
    "                    maxErrNameMap[(lab, plab)] = errFIdx\n",
    "    title_font = {'fontname':'Arial', 'size':'16', \n",
    "                  'color':'black', 'weight':'bold'} \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(25):\n",
    "        rIdx = i // 5\n",
    "        cIdx = i % 5\n",
    "        ax = plt.subplot(5, 5, i+1)\n",
    "        if rIdx == 0:\n",
    "            ax.xaxis.set_label_position('top') \n",
    "            ax.set_xlabel( label_names[int(label_set[cIdx])].decode('utf-8'), \n",
    "                           rotation=45, rotation_mode='anchor', ha = 'left' )\n",
    "        if cIdx == 0:\n",
    "            ax.yaxis.set_label_position('left')\n",
    "            ax.set_ylabel( label_names[int(label_set[rIdx])].decode('utf-8'),  \n",
    "                           rotation=45, rotation_mode='anchor', ha = 'right' )\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        if (label_set[rIdx], label_set[cIdx]) in maxErrNameMap:\n",
    "            fIdx = maxErrNameMap[(label_set[rIdx], label_set[cIdx])]            \n",
    "            folderName = label_names[test_labels[fIdx]]            \n",
    "            fID = test_sample_ids[fIdx]\n",
    "            img=mpimg.imread( srcPath + '/' + folderName.decode('utf-8') + '/' + \\\n",
    "                             fID.decode('utf-8') + '.jpg')    \n",
    "            plt.imshow(img)  # The AxesGrid object work as a list of axes.\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, cnf_matrix[rIdx, cIdx],\n",
    "                     **title_font)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = np.unique(test_labels)\n",
    "gen_err_predicted_fig( test_labels, pred_lab, label_set, label_names, test_sample_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#The-plan:\">top</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
